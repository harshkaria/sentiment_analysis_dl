{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon Review Classification Using Statistical Learning and Deep Learning\n",
        "\n",
        "In this notebook, we examine review classification via a variety of methods.\n",
        "\n",
        "## Word2Vec and Similarity Comparisons\n",
        "We begin by examining Word2Vec. Word2Vec is a library that allows us to train on our own corpus to find embeddings of words. First, we examine the GoogleNews-300 vector dataset which is a pretrained embeddings corpus based on Google News. \n",
        "\n",
        "We compute similarities on phrases such as `Awesome ~ Amazing and excellent ~ outstanding` on the Google News corpus and the reviews corpus and we find the following results\n",
        "\n",
        "Google News Corpus\n",
        "\n",
        "*   Awesome ~ Amazing: Awesome is 82.82865285873413% similar to Amazing \n",
        "*   excellent ~ outstanding: Awesome is Excellent is 55.67485690116882% similar to Outstanding \n",
        "\n",
        "Amazon Corpus:\n",
        "\n",
        "\n",
        "*   Awesome ~ Amazing: Awesome is 77.89764404296875% similar to Amazing \n",
        "*   excellent ~ outstanding: Excellent is 63.154661655426025% similar to Outstanding \n",
        "\n",
        "For one of the words above, the dataset we provided is better. This is because we trained on a corpus of reviews, and the words we tested on correspond to sentiment words.\n",
        "\n",
        "The reviews corpus is only trained on a small subset of reviews and thus has a smaller vocabulary (so it might fail when a word we encounter is not within the corpus); in a broader sense, it is best to use the Google News Corpus\n",
        "\n",
        "## Training Simple Models\n",
        "Before we train simple models, we take the average embeddings of all of the words in a review and use that as our embeddings per review and perform a 80-20 split of train/test data. \n",
        "\n",
        "## Perceptron\n",
        "We train a perceptron model on the averaged embeddings and find a **60.63%** accuracy\n",
        "\n",
        "## Linear SVM\n",
        "We train a linear SVM model on the averaged embeddings and find a **68.18%** accuracy\n",
        "\n",
        "### Accuracy Comparisons with TFIDF\n",
        "*   Perceptron (TFIDF): 64.06%\n",
        "*   Perceptron (Word2Vec): 60.63%\n",
        "*   SVM (TFIDF): 70.58%\n",
        "*   SVM (Word2Vec): 68.18%\n",
        "\n",
        "\n",
        "When training using the perceptron, TFIDF performs better than Word2Vec (64.06% using TFIDF, 60.6% using Word2Vec)\n",
        "\n",
        "Similarly, training using the perceptron, TFIDF performs better than Word2Vec (70.58% using TFIDF, 68.18% using Word2Vec)\n",
        "\n",
        "Overall, TFIDF performs better. \n",
        "\n",
        "Both show improvement when using SVM. The perceptron may be overfitting and thus the SVM with the features averaged performs better\n",
        "\n",
        "## Feedforward Neural Network\n",
        "A feedforward neural network is a network with hidden layers that train in one direction without aa hidden state capturing information about thue past. FFNNs are typically used for tasks such as image classification, text classification, and regression, where the input data is fixed in size and there is no temporal relationship between observations.\n",
        "\n",
        "We train a feedforward neural network with 2 hidden layers and each with 100 and 10 nodes, respectively. We use our averaged embeddings as features from earlier. We use cross-entropy as our loss function along with an Adam Optimizer with betas (0.9, 0.999) and a learning rate of 0.001 and train for 20 epochs.\n",
        "\n",
        "We obtain an accuracy of **69.34%** using the configuration described above.\n",
        "\n",
        "### First 10 words\n",
        "We then condense our features and only use the first 10 words of every review. \n",
        "\n",
        "We obtain an accuracy of **59.56%** using only the first 10 words, which intuitively makes sense since we are losing context.\n",
        "\n",
        "\n",
        "## Recurrent Neural Network\n",
        "Recurrent neural networks introduce the context of \"hidden states\" which allow them to incorporate the idea of sequential memory. The hidden state is updated at each time step and at each iteration the hidden state from the previous time step is used as input to the network alongside the input at the current time step.\n",
        "\n",
        "We use the embeddings of the first 20 words of the review, truncating the review if longer or padding with 0 if shorter as our features.\n",
        "\n",
        "Our RNN has an input_size of 300 dimensions, a hidden dimension of size 20, and a batch size of 32 with an Adam Optimizer with betas (0.9, 0.999) and a learning rate of 0.001 and train for 20 epochs, using cross entropy as our loss function. \n",
        "\n",
        "We obtain an accuracy of **58.00%** on the test set after training the simple RNN.\n",
        "\n",
        "## Gated Recurrent Unit (GRU)\n",
        "GRUs have the concepts of an \"update\" gate and a \"reset\" gate which control how much of the current state to use and how much of the past information is important. The update gate z_t controls how much of the new information to incorporate into the hidden state, while the reset gate r_t controls how much of the previous hidden state to forget.\n",
        "\n",
        "The gates are defined as follows:\n",
        "\n",
        "z_t = sigmoid(W_xz * x_t + W_hz * h_t-1) <br>\n",
        "r_t = sigmoid(W_xr * x_t + W_hr * h_t-1)\n",
        "\n",
        "where W_xz, W_hz, W_xr, and W_hr are weight matrices that determine the contribution of the input and the previous hidden state to each gate, and sigmoid is the sigmoid activation function.\n",
        "\n",
        "Our GRU has an input_size of 3 layers, 300 dimensions, a hidden dimension of size 20, and a batch size of 32 with an Adam Optimizer with betas (0.9, 0.999) and a learning rate of 0.001 and train for 20 epochs, using cross entropy as our loss function. \n",
        "\n",
        "We obtain an accuracy of **59.04%** on the test set after training the GRU.\n",
        "\n",
        "## LSTM\n",
        "LSTMs are similar to GRUs, but have an input gate, a forget gate, and an output gate. . At each time step t, the model receives an input x_t and computes the activations of the gates and the new cell state c_t~ with the weights and biases of the hidden state and all the gates. \n",
        "\n",
        "The input gate i_t controls how much of the new input to incorporate into the new cell state c_t , while the forget gate f_t controls how much of the previous cell state c_t-1 to forget. The output gate o_t determines how much of the new cell state to output as the hidden state h_t. The cell state c_t is updated as a combination of the previous cell state c_t-1 and the new cell state c_t~.\n",
        "\n",
        "By using these gates and the cell state, the LSTM architecture can selectively learn and forget information over time, allowing it to capture long-term dependencies more effectively. The input gate i_t and the forget gate f_t determine how much information to store in the cell state, while the output gate o_t determines how much of the stored information to output as the hidden state h_t. The hidden state can be seen as the \"summary\" of the previous inputs, which is then used as input to the next time step. \n",
        "\n",
        "The cell state can be seen as the \"long-term memory\" of the model, which can store information over multiple time steps and selectively forget or update it based on the activations of the gates.\n",
        "\n",
        "Our LSTM has an input_size of 4 layers, 300 dimensions, a hidden dimension of size 20, and a batch size of 32 with an Adam Optimizer with betas (0.9, 0.999) and a learning rate of 0.001 and train for 20 epochs, using cross entropy as our loss function. \n",
        "\n",
        "We obtain an accuracy of **60.75%** on the test set after training the LSTM.\n",
        "\n"
      ],
      "metadata": {
        "id": "lJfcUkBdrzEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "_tsRYQ2MceN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = 'colab'"
      ],
      "metadata": {
        "id": "uvZrXTv4ckYj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cx2Jt0O-cVQ2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
        "!wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
        "!gunzip /content/amazon_reviews_us_Beauty_v1_00.tsv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RW4ANOWco8W",
        "outputId": "91f959c8-4b89-495e-aaf3-e3ed11879222"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-24 20:36:23--  https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.172.72, 52.216.166.109, 52.216.245.14, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.172.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914070021 (872M) [application/x-gzip]\n",
            "Saving to: ‘amazon_reviews_us_Beauty_v1_00.tsv.gz’\n",
            "\n",
            "amazon_reviews_us_B 100%[===================>] 871.72M  12.9MB/s    in 75s     \n",
            "\n",
            "2023-02-24 20:37:39 (11.6 MB/s) - ‘amazon_reviews_us_Beauty_v1_00.tsv.gz’ saved [914070021/914070021]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import nltk\n",
        "# import re\n",
        "# from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "35MWJPp9qWmD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = None\n",
        "if env == \"colab\":\n",
        "   dataset_path = './amazon_reviews_us_Beauty_v1_00.tsv'\n",
        "else:\n",
        "    dataset_path = './data.tsv'"
      ],
      "metadata": {
        "id": "6FYBlmqJct2U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVfNNREhXyyv",
        "outputId": "3c98c6a8-6672-44c6-ebbe-5ec7ba23ff7b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
      ],
      "metadata": {
        "id": "XQIF_4Odgd4m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings\n",
        "Generate word2vec embeddings- learning semantic and syntactic relationships between words"
      ],
      "metadata": {
        "id": "MhgJpMjec5jA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking semantic similarities on word2vec-google-news-300 \n",
        "On phrases such as `Awesome ~ Amazing and excellent ~ outstanding`"
      ],
      "metadata": {
        "id": "0A9crB0ZiLZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_excellent = wv.similarity('excellent', 'outstanding')\n",
        "print(f'Excellent is {similar_excellent * 100}% similar to Outstanding ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9SNJpGBop22",
        "outputId": "60633eeb-2764-44a9-e863-e8b1c68b80b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent is 55.67485690116882% similar to Outstanding \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_awesome = wv.similarity('awesome', 'amazing')\n",
        "print(f'Awesome is {similar_awesome * 100}% similar to Amazing ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yp099NQuamU",
        "outputId": "605ac1b0-8ea4-458b-b8fc-826ff7d19f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Awesome is 82.82865285873413% similar to Amazing \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Word2Vec on Amazon Reviews Dataset"
      ],
      "metadata": {
        "id": "N9KPrc3sphiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Dataset\n",
        "Removing HTML, URLs, converting floats to strings, dropping all columns except for the reviews"
      ],
      "metadata": {
        "id": "wsrEF3jVqm5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFrAIvw-pcQo",
        "outputId": "edd7fef0-db36-4248-dc84-016e162fb3c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.read_csv(dataset_path, sep='\\t', on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "tXXD18Mwq8Rw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.drop(merged_df.columns.difference(['review_body', 'star_rating']), 1, inplace=True)"
      ],
      "metadata": {
        "id": "B3OnZouJwRKm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.loc[merged_df['star_rating'] == '1', 'class'] = 1\n",
        "merged_df.loc[merged_df['star_rating'] == '1', 'class'] = 1\n",
        "merged_df.loc[merged_df['star_rating'] == '3', 'class'] = 2\n",
        "merged_df.loc[merged_df['star_rating'] == '4', 'class'] = 3\n",
        "merged_df.loc[merged_df['star_rating'] == '5', 'class'] = 3"
      ],
      "metadata": {
        "id": "c7hiTV6y-nfz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[\"review_body\"] = merged_df[\"review_body\"].apply(lambda x: str(x) if type(x)==float else x)"
      ],
      "metadata": {
        "id": "ymOTEbKBwvfY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class1_sample = merged_df.loc[merged_df[\"class\"] == 1].sample(n = 20000)\n",
        "class2_sample = merged_df.loc[merged_df[\"class\"] == 2].sample(n = 20000)\n",
        "class3_sample = merged_df.loc[merged_df[\"class\"] == 3].sample(n = 20000)"
      ],
      "metadata": {
        "id": "yHZK7MBDgsjK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat([class1_sample, class2_sample, class3_sample], axis=0)"
      ],
      "metadata": {
        "id": "Q0EzRidjg0Sj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = merged_df[\"review_body\"].values.tolist()"
      ],
      "metadata": {
        "id": "frXNqd4l3ppR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_reviews = [review.lower().split() for review in reviews]"
      ],
      "metadata": {
        "id": "lWaZlMe3xEi5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.models"
      ],
      "metadata": {
        "id": "buiNjiTN8dWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(sentences=tokenized_reviews, size=300, window = 13, min_count=9)"
      ],
      "metadata": {
        "id": "slOI0p-88oH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_excellent_rev = model.similarity('excellent', 'outstanding')\n",
        "print(f'Excellent is {similar_excellent_rev * 100}% similar to Outstanding ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEsSVp3Pwm4x",
        "outputId": "c1c9a00f-014c-40ee-9668-c2b4d81f8a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent is 63.154661655426025% similar to Outstanding \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_awesome_rev = model.similarity('awesome', 'amazing')\n",
        "print(f'Awesome is {similar_awesome_rev * 100}% similar to Amazing ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Cqz9hQxhR3",
        "outputId": "c9bad184-1974-4845-d588-b3f2b4952c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Awesome is 77.89764404296875% similar to Amazing \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one of the words above, the dataset we provided is better. This is because we trained on a corpus of reviews, and the words we tested on correspond to sentiment words. <br>\n",
        "\n",
        "The reviews corpus is only trained on a small subset of reviews and thus has a smaller vocabulary (so it might fail when a word we encounter is not within the corpus); in a broader sense, it is best to use the Google News Corpus"
      ],
      "metadata": {
        "id": "qtA6FbjTzvy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training simple models"
      ],
      "metadata": {
        "id": "YVLtCUyr0llW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "toLAMatJBsOG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_Y = merged_df[\"class\"].values\n",
        "labels_Y = labels_Y[:, np.newaxis]"
      ],
      "metadata": {
        "id": "TeOFqIAOiAgi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_vectors(reviews, model):\n",
        "  reviews_X = np.empty((60000, 300))\n",
        "  for idx, review in enumerate(reviews):\n",
        "    vectors = [model[word] if word in model else 0.0 for word in review]\n",
        "    avg_vectors = np.mean(vectors, axis=0)\n",
        "    reviews_X[idx] = avg_vectors\n",
        "  return reviews_X"
      ],
      "metadata": {
        "id": "gkrC3-RYQQAk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_X = average_vectors(tokenized_reviews, wv)"
      ],
      "metadata": {
        "id": "2jXxM3pcRNnh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(reviews_X, labels_Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "klHUycHph6ye"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes of the resulting matrices\n",
        "if env == 'colab':\n",
        "  print(\"Shape of X_train:\", X_train.shape)\n",
        "  print(\"Shape of X_test:\", X_test.shape)\n",
        "  print(\"Shape of y_train:\", y_train.shape)\n",
        "  print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BxixOkijq8o",
        "outputId": "164b00da-f0bc-460c-cd6a-b68d61161a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (48000, 300)\n",
            "Shape of X_test: (12000, 300)\n",
            "Shape of y_train: (48000, 1)\n",
            "Shape of y_test: (12000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron Model with Word2Vec"
      ],
      "metadata": {
        "id": "bCnTIB0ViiQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "perceptron = Perceptron(random_state=0)\n",
        "\n",
        "perceptron.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaoE0O9filgz",
        "outputId": "78efc415-7680-40a5-f591-ed73b0179afa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron()"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the testing data\n",
        "y_pred_perceptron = perceptron.predict(X_test)"
      ],
      "metadata": {
        "id": "E81C3_6ToC23"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_perceptron = accuracy_score(y_test, y_pred_perceptron)\n",
        "print(f'Perceptron accuracy: {acc_perceptron}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPBrPosXoq7d",
        "outputId": "b9bc53a8-61f1-445e-b5b1-6cfb8f70f039"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron accuracy: 0.6063333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM with Word2Vec"
      ],
      "metadata": {
        "id": "1fmu80buva9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(random_state=0, max_iter=1000).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "S9cRoINRvcvs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pred = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "RVQu-ItVvhqE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_svm = accuracy_score(y_test, svm_pred)\n",
        "print(f'SVM accuracy: {acc_svm}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oqDorK3LOve",
        "outputId": "2e8a7d31-d6f1-4e6a-b9f2-2382cea09a31"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy: 0.6818333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training using the perceptron, TFIDF performs better than Word2Vec (64.06% using TFIDF, 60.6% using Word2Vec)\n",
        "\n",
        "Similarly, training using the perceptron, TFIDF performs better than Word2Vec (70.58% using TFIDF, 68.18% using Word2Vec)\n",
        "\n",
        "Both show improvement when using SVM. The perceptron may be overfitting and thus the SVM with the features averaged performs better"
      ],
      "metadata": {
        "id": "SA6s-HHSwIk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward Neural Network"
      ],
      "metadata": {
        "id": "ah7D5MPEvg6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we examine using a feedforward neural network using the average of the word2vec vectors, similar to above."
      ],
      "metadata": {
        "id": "uZhNTJ-8vonK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the SKLearn data to PyTorch dataloader, curteousy of https://www.kaggle.com/code/glebbuzin/solving-sklearn-datasets-with-pytorch"
      ],
      "metadata": {
        "id": "J4gsbWy1ws_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, num_classes):\n",
        "  y_t = torch.transpose(y, 0, 1).squeeze() \n",
        "  one_hot = torch.nn.functional.one_hot(y_t, num_classes)\n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "xk2716qKL-W5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "rbzlzeUMxKpC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X_train_t = torch.from_numpy(X_train).to(torch.float32)\n",
        "y_train_t = torch.from_numpy(y_train).to(torch.long)\n",
        "X_test_t = torch.from_numpy(X_test).to(torch.float32)\n",
        "y_test_t = torch.from_numpy(y_test).to(torch.long)"
      ],
      "metadata": {
        "id": "-OQc2DhJv445"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t = one_hot(y_train_t, 4)\n",
        "y_test_t = one_hot(y_test_t, 4)"
      ],
      "metadata": {
        "id": "K1RC4RHhVvpz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "zf1X2PzYxCcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.in1 = nn.Linear(input_size, 100)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.l2 = nn.Linear(100, 10)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.out = nn.Linear(10, output_size)\n",
        "  def forward(self, x):\n",
        "    y = self.in1(x)\n",
        "    y = self.relu1(y)\n",
        "    y = self.l2(y)\n",
        "    y = self.relu2(y)\n",
        "    y = self.out(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "wzo-1dZNxf5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CMWAGe-r6sm5",
        "outputId": "2376314c-ad22-467a-c7ac-b2118964f5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeedForwardNN(300, 4).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATfUuZ0q7DFM",
        "outputId": "a03dc02e-2eaf-4fb2-e6b1-edc7b92f037b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN(\n",
            "  (in1): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (l2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (out): Linear(in_features=10, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "class Trainer:\n",
        "  def __init__(self, model, training_data, device):\n",
        "    self.hyperparams = {\n",
        "        'lr': 1e-3,\n",
        "        'epochs': 20,\n",
        "        'betas': (0.9, 0.999)\n",
        "    }\n",
        "    self.td = training_data\n",
        "    self.device = device\n",
        "    self.loss_fn = nn.CrossEntropyLoss()\n",
        "    self.model = model\n",
        "    self.optim = torch.optim.Adam(self.model.parameters(), lr = self.hyperparams['lr'], betas=(self.hyperparams['betas']))\n",
        "  \n",
        "  def calc_accuracy(self, output, y):\n",
        "    pred = torch.argmax(output, dim=1)\n",
        "    y = torch.argmax(y, dim=1)\n",
        "    return (pred == y).sum().item() / len(y)\n",
        "  \n",
        "  def calc_accuracy_test(self, output, y):\n",
        "    pred = torch.argmax(output, dim=1)\n",
        "    return (pred == y).sum().item() / len(y)\n",
        "  \n",
        "  def train_epoch(self):\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "\n",
        "    for review, label in self.td:\n",
        "      self.optim.zero_grad()\n",
        "      review = review.to(self.device, dtype=torch.float32)\n",
        "      label = label.to(self.device, dtype=torch.float32)\n",
        "      output = self.model(review)\n",
        "      if isinstance(output, tuple):\n",
        "        output = output[0]\n",
        "\n",
        "      loss = self.loss_fn(output, label)\n",
        "\n",
        "      loss.backward()\n",
        "      self.optim.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      running_acc += self.calc_accuracy(output, label)\n",
        "\n",
        "      del review, label, output\n",
        "\n",
        "    train_loss = running_loss / len(self.td)\n",
        "    training_acc = running_acc / len(self.td)\n",
        "    return train_loss, training_acc\n",
        "  \n",
        "  def fit(self):        \n",
        "    train_losses,train_accs = [], []\n",
        "    min_vl = 999999\n",
        "\n",
        "    for epoch in range(self.hyperparams['epochs']):\n",
        "        \n",
        "        print(f\"------EPOCH {epoch+1}/{self.hyperparams['epochs']}------\")\n",
        "        \n",
        "        self.model.train()\n",
        "        \n",
        "        train_loss, train_acc = self.train_epoch()\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        \n",
        "\n",
        "        \n",
        "        print(f\"Training LOSS: {train_loss} | ACCURACY: {train_acc}\")\n",
        "        \n",
        "        \n",
        "        \n",
        "        # CLEANUP\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "\n",
        "    return (train_losses, train_accs)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "YXSr9M6j9vOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, train_dataloader, device)"
      ],
      "metadata": {
        "id": "45GXVB6c-a9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses, train_accs) = trainer.fit()"
      ],
      "metadata": {
        "id": "Aj0GR_m5YS_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c11861-aa19-4e2e-f80b-53a4c25e7f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/20------\n",
            "Training LOSS: 0.8580149493018786 | ACCURACY: 0.616\n",
            "------EPOCH 2/20------\n",
            "Training LOSS: 0.7594271267851194 | ACCURACY: 0.6742708333333334\n",
            "------EPOCH 3/20------\n",
            "Training LOSS: 0.7386357170740764 | ACCURACY: 0.6841041666666666\n",
            "------EPOCH 4/20------\n",
            "Training LOSS: 0.7250249315301577 | ACCURACY: 0.6900416666666667\n",
            "------EPOCH 5/20------\n",
            "Training LOSS: 0.7146575082937876 | ACCURACY: 0.6951666666666667\n",
            "------EPOCH 6/20------\n",
            "Training LOSS: 0.7060685671965281 | ACCURACY: 0.6981666666666667\n",
            "------EPOCH 7/20------\n",
            "Training LOSS: 0.6984484259088835 | ACCURACY: 0.7018125\n",
            "------EPOCH 8/20------\n",
            "Training LOSS: 0.6910099113980929 | ACCURACY: 0.704875\n",
            "------EPOCH 9/20------\n",
            "Training LOSS: 0.6836891483068466 | ACCURACY: 0.7082083333333333\n",
            "------EPOCH 10/20------\n",
            "Training LOSS: 0.6768757694164912 | ACCURACY: 0.7116875\n",
            "------EPOCH 11/20------\n",
            "Training LOSS: 0.670400546014309 | ACCURACY: 0.7146458333333333\n",
            "------EPOCH 12/20------\n",
            "Training LOSS: 0.664261295457681 | ACCURACY: 0.717625\n",
            "------EPOCH 13/20------\n",
            "Training LOSS: 0.6583183747132619 | ACCURACY: 0.7194166666666667\n",
            "------EPOCH 14/20------\n",
            "Training LOSS: 0.6524098198612531 | ACCURACY: 0.7231666666666666\n",
            "------EPOCH 15/20------\n",
            "Training LOSS: 0.6463683868646621 | ACCURACY: 0.7254166666666667\n",
            "------EPOCH 16/20------\n",
            "Training LOSS: 0.6405979769627254 | ACCURACY: 0.729125\n",
            "------EPOCH 17/20------\n",
            "Training LOSS: 0.6347677610516548 | ACCURACY: 0.7310625\n",
            "------EPOCH 18/20------\n",
            "Training LOSS: 0.6290334487557411 | ACCURACY: 0.7341041666666667\n",
            "------EPOCH 19/20------\n",
            "Training LOSS: 0.623329409678777 | ACCURACY: 0.7363541666666666\n",
            "------EPOCH 20/20------\n",
            "Training LOSS: 0.6175186781485875 | ACCURACY: 0.7394375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def infer(trainer, model, test_dataloader):\n",
        "    \n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    test_output = []\n",
        "    true_res = []\n",
        "    incorrect_examples = []\n",
        "    incorrect_labels = []\n",
        "    incorrect_pred = []\n",
        "    \n",
        "    \n",
        "    for x,y in test_dataloader:\n",
        "        \n",
        "        x = x.to(device, dtype=torch.float)\n",
        "        y = y.to(device, dtype=torch.float)\n",
        "        \n",
        "        output = model(x)\n",
        "        if isinstance(output, tuple):\n",
        "          output = output[0]\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        y = torch.argmax(y, dim=1)\n",
        "        test_output.append(pred)\n",
        "        true_res.append(y)\n",
        "\n",
        "        idxs_mask = ((pred == y.view_as(pred))==False).view(-1)\n",
        "        if idxs_mask.numel(): #if index masks is non-empty append the correspoding data value in incorrect examples\n",
        "          incorrect_examples.append(x[idxs_mask].squeeze().cpu().numpy())\n",
        "          incorrect_labels.append(y[idxs_mask].cpu().numpy()) #the corresponding target to the misclassified image\n",
        "          incorrect_pred.append(pred[idxs_mask].squeeze().cpu().numpy()) #the corresponiding predicted class of the misclassified image\n",
        "        \n",
        "        loss = trainer.loss_fn(output, y)\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        running_acc += trainer.calc_accuracy_test(output,y)\n",
        "        \n",
        "        del x,y,output\n",
        "        \n",
        "    test_loss = running_loss/len(test_dataloader)\n",
        "    test_acc = running_acc/len(test_dataloader)\n",
        "    \n",
        "    return test_loss, test_acc, test_output, true_res, incorrect_examples, incorrect_labels, incorrect_pred\n"
      ],
      "metadata": {
        "id": "zZk8D7wCUdaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_plain, test_acc_plain, test_out, true_res, incorrect_examples, incorrect_labels, incorrect_pred = infer(model, test_dataloader)"
      ],
      "metadata": {
        "id": "X-sbUjChVD47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Loss: {test_loss_plain}')\n",
        "print(f'Test Accuracy: {test_acc_plain}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocyn5_GOYYz9",
        "outputId": "e8f76dd8-44d1-4e1c-a135-3d737fcc9bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7319384795029958\n",
            "Test Accuracy: 0.6934166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only the first 10 words\n"
      ],
      "metadata": {
        "id": "uYIlg62ta_Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topten_reviews(reviews, model):\n",
        "    num_reviews = len(reviews)\n",
        "    vector_size = model.vector_size\n",
        "    reviews_X = np.zeros((num_reviews, 10, vector_size))\n",
        "    for i, review in enumerate(reviews):\n",
        "        num_words = min(len(review), 10)\n",
        "        for j in range(num_words):\n",
        "            word = review[j]\n",
        "            if word in model:\n",
        "                reviews_X[i,j,:] = model[word]\n",
        "    return reviews_X"
      ],
      "metadata": {
        "id": "LowHjoQvCoU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_top10 = topten_reviews(tokenized_reviews, wv)"
      ],
      "metadata": {
        "id": "TxqeRQJ5DARn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t10, X_test_t10, y_train_t10, y_test_t10 = train_test_split(reviews_top10, labels_Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "iELiQP7xGJhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X_train_reduced_t = torch.from_numpy(X_train_t10).to(torch.float32)\n",
        "y_train_t_reduced = torch.from_numpy(y_train_t10).to(torch.long)\n",
        "X_test_reduced_t = torch.from_numpy(X_test_t10).to(torch.float32)\n",
        "y_test_t_reduced = torch.from_numpy(y_test_t10).to(torch.long)"
      ],
      "metadata": {
        "id": "pIITJ16aba-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t_reduced = one_hot(y_train_t_reduced, 4)\n",
        "y_test_t_reduced = one_hot(y_test_t_reduced, 4)"
      ],
      "metadata": {
        "id": "EeTpGshid5Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes of the resulting matrices\n",
        "if env == 'colab':\n",
        "  print(\"Shape of X_train:\", X_train_reduced_t.shape)\n",
        "  print(\"Shape of X_test:\", X_test_reduced_t.shape)\n",
        "  print(\"Shape of y_train:\", y_train_t_reduced.shape)\n",
        "  print(\"Shape of y_test:\", y_test_t_reduced.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA6v22eubtIf",
        "outputId": "e4b0abb5-0616-41a9-a089-ac66178ff852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: torch.Size([48000, 10, 300])\n",
            "Shape of X_test: torch.Size([12000, 10, 300])\n",
            "Shape of y_train: torch.Size([48000, 4])\n",
            "Shape of y_test: torch.Size([12000, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_reduced = TensorDataset(X_train_reduced_t, y_train_t_reduced)\n",
        "test_dataset_reduced = TensorDataset(X_test_reduced_t, y_test_t_reduced)\n",
        "train_dataloader_reduced = DataLoader(train_dataset_reduced, batch_size=32)\n",
        "test_dataloader_reduced = DataLoader(test_dataset_reduced, batch_size=32)"
      ],
      "metadata": {
        "id": "WRI6KLRecK6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNNTopX(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(FeedForwardNNTopX, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.in1 = nn.Linear(input_size, 100)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.l2 = nn.Linear(100, 10)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.out = nn.Linear(10, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = x.view(batch_size, -1)  # Reshape input to (batch_size, input_size)\n",
        "    y = self.in1(x)\n",
        "    y = self.relu1(y)\n",
        "    y = self.l2(y)\n",
        "    y = self.relu2(y)\n",
        "    y = self.out(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "hX8CdCYALZxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_reduced = FeedForwardNNTopX(3000, 4).to(device)\n",
        "print(model_reduced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtnajLdqdXvT",
        "outputId": "1aec8af3-b532-4f7f-9541-0cd305ec0301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNNTop10(\n",
            "  (in1): Linear(in_features=3000, out_features=100, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (l2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (out): Linear(in_features=10, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_reduced = Trainer(model_reduced, train_dataloader_reduced, device)"
      ],
      "metadata": {
        "id": "JTbHi4ubcjSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses, train_accs) = trainer_reduced.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW5TGjHWc8LY",
        "outputId": "4ea9b10d-87d7-4aff-d469-d8601af69f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/20------\n",
            "Training LOSS: 0.8609319173494975 | ACCURACY: 0.5948541666666667\n",
            "------EPOCH 2/20------\n",
            "Training LOSS: 0.7586062659819921 | ACCURACY: 0.6608125\n",
            "------EPOCH 3/20------\n",
            "Training LOSS: 0.6888598437905311 | ACCURACY: 0.7007708333333333\n",
            "------EPOCH 4/20------\n",
            "Training LOSS: 0.6045685842235883 | ACCURACY: 0.7456458333333333\n",
            "------EPOCH 5/20------\n",
            "Training LOSS: 0.5123702744940917 | ACCURACY: 0.79275\n",
            "------EPOCH 6/20------\n",
            "Training LOSS: 0.42147025881210964 | ACCURACY: 0.836\n",
            "------EPOCH 7/20------\n",
            "Training LOSS: 0.33807916860779125 | ACCURACY: 0.8742708333333333\n",
            "------EPOCH 8/20------\n",
            "Training LOSS: 0.2696292301391562 | ACCURACY: 0.9031458333333333\n",
            "------EPOCH 9/20------\n",
            "Training LOSS: 0.22766238913064202 | ACCURACY: 0.9176666666666666\n",
            "------EPOCH 10/20------\n",
            "Training LOSS: 0.20108065392325322 | ACCURACY: 0.9254375\n",
            "------EPOCH 11/20------\n",
            "Training LOSS: 0.1858758095347633 | ACCURACY: 0.9307291666666667\n",
            "------EPOCH 12/20------\n",
            "Training LOSS: 0.17518385212744275 | ACCURACY: 0.9336875\n",
            "------EPOCH 13/20------\n",
            "Training LOSS: 0.153077661447227 | ACCURACY: 0.9425416666666667\n",
            "------EPOCH 14/20------\n",
            "Training LOSS: 0.1426060023525109 | ACCURACY: 0.9469166666666666\n",
            "------EPOCH 15/20------\n",
            "Training LOSS: 0.13537535608249407 | ACCURACY: 0.948625\n",
            "------EPOCH 16/20------\n",
            "Training LOSS: 0.12654574104895194 | ACCURACY: 0.951625\n",
            "------EPOCH 17/20------\n",
            "Training LOSS: 0.1271888968044271 | ACCURACY: 0.952\n",
            "------EPOCH 18/20------\n",
            "Training LOSS: 0.11879254609222213 | ACCURACY: 0.9549583333333334\n",
            "------EPOCH 19/20------\n",
            "Training LOSS: 0.11640622439545889 | ACCURACY: 0.9562708333333333\n",
            "------EPOCH 20/20------\n",
            "Training LOSS: 0.1077226959693556 | ACCURACY: 0.9605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_plain_r, test_acc_plain_r, test_out_r, true_res_r, incorrect_examples_r, incorrect_labels_r, incorrect_pred_r = infer(trainer_reduced, model_reduced, test_dataloader_reduced)"
      ],
      "metadata": {
        "id": "xeiLhQa6e2Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Loss (on first 10): {test_loss_plain_r}')\n",
        "print(f'Test Accuracy (on first 10): {test_acc_plain_r}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3xTpSCYfEdV",
        "outputId": "1172d205-3fe5-4fef-b77d-56ac27278192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (on first 10): 3.2353582534790037\n",
            "Test Accuracy (on first 10): 0.5956666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Network"
      ],
      "metadata": {
        "id": "qVZu2Aww0jj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/"
      ],
      "metadata": {
        "id": "tlcNZXkTCXFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_all_reviews(reviews, model, rev_len):\n",
        "    num_reviews = len(reviews)\n",
        "    vector_size = model.vector_size\n",
        "    reviews_X = np.zeros((num_reviews, rev_len, vector_size))\n",
        "    for i, review in enumerate(reviews):\n",
        "        num_words = len(review)\n",
        "        for j in range(min(num_words, rev_len)):\n",
        "            word = review[j]\n",
        "            if word in model:\n",
        "                reviews_X[i,j,:] = model[word]\n",
        "    return reviews_X"
      ],
      "metadata": {
        "id": "oZiBGeQDOSKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_f20_rnn_tokenized = tokenize_all_reviews(reviews, wv, 20)"
      ],
      "metadata": {
        "id": "3Rb6_qenOU8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_f20, X_test_f20, y_train_f20, y_test_f20 = train_test_split(X_f20_rnn_tokenized, labels_Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AT_s4twgbNnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up memory\n",
        "reviews = None\n",
        "wv = None\n",
        "labels_Y = None\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvSbapRjeGZN",
        "outputId": "46122cbb-ccf4-4c16-deeb-593ae166c102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X_train_t_rnn = torch.from_numpy(X_train_f20).to(torch.float32)\n",
        "y_train_t_rnn = torch.from_numpy(y_train_f20).to(torch.long)\n",
        "X_test_t_rnn = torch.from_numpy(X_test_f20).to(torch.float32)\n",
        "y_test_t_rnn = torch.from_numpy(y_test_f20).to(torch.long)"
      ],
      "metadata": {
        "id": "HCpKbi7QbAtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t_rnn = one_hot(y_train_t_rnn, 4)\n",
        "y_test_t_rnn = one_hot(y_test_t_rnn, 4)"
      ],
      "metadata": {
        "id": "ZZtvR58ffpkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_rnn = TensorDataset(X_train_t_rnn, y_train_t_rnn)\n",
        "test_dataset_rnn = TensorDataset(X_test_t_rnn, y_test_t_rnn)\n",
        "train_dataloader_rnn = DataLoader(train_dataset_rnn, batch_size=32)\n",
        "test_dataloader_rnn = DataLoader(test_dataset_rnn, batch_size=32)"
      ],
      "metadata": {
        "id": "FEb8AYdoIinn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 20, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size).to(device)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = out.contiguous().view(batch_size, -1)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "dy3pARIl0nYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplernn_model = SimpleRNN(300, 4, 20, 3).to(device)\n",
        "print(simplernn_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97fTUp-iDznP",
        "outputId": "a54592cd-1988-4831-9f61-1d8d96313a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleRNN(\n",
            "  (rnn): RNN(300, 20, num_layers=3, batch_first=True)\n",
            "  (fc): Linear(in_features=400, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_rnn = Trainer(simplernn_model, train_dataloader_rnn, device)"
      ],
      "metadata": {
        "id": "DUGBReb-IrnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses_rnn, train_accs_rnn) = trainer_rnn.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBy_0kPaKVnc",
        "outputId": "a08a6a39-2445-47d7-c5b4-f5a722507542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/20------\n",
            "Training LOSS: 1.0060612388451895 | ACCURACY: 0.478375\n",
            "------EPOCH 2/20------\n",
            "Training LOSS: 0.9562386382420858 | ACCURACY: 0.5243125\n",
            "------EPOCH 3/20------\n",
            "Training LOSS: 0.9332923852205276 | ACCURACY: 0.54175\n",
            "------EPOCH 4/20------\n",
            "Training LOSS: 0.9160294694900513 | ACCURACY: 0.5532083333333333\n",
            "------EPOCH 5/20------\n",
            "Training LOSS: 0.9019498533805211 | ACCURACY: 0.5620208333333333\n",
            "------EPOCH 6/20------\n",
            "Training LOSS: 0.8890658142169316 | ACCURACY: 0.5714583333333333\n",
            "------EPOCH 7/20------\n",
            "Training LOSS: 0.8777119198242823 | ACCURACY: 0.5814166666666667\n",
            "------EPOCH 8/20------\n",
            "Training LOSS: 0.8680261938969295 | ACCURACY: 0.58875\n",
            "------EPOCH 9/20------\n",
            "Training LOSS: 0.8595240587393442 | ACCURACY: 0.593625\n",
            "------EPOCH 10/20------\n",
            "Training LOSS: 0.8519853099981943 | ACCURACY: 0.5967708333333334\n",
            "------EPOCH 11/20------\n",
            "Training LOSS: 0.8452907829284668 | ACCURACY: 0.6011458333333334\n",
            "------EPOCH 12/20------\n",
            "Training LOSS: 0.8394368140697479 | ACCURACY: 0.6051875\n",
            "------EPOCH 13/20------\n",
            "Training LOSS: 0.8343056011199951 | ACCURACY: 0.608625\n",
            "------EPOCH 14/20------\n",
            "Training LOSS: 0.8297795467376708 | ACCURACY: 0.6120208333333333\n",
            "------EPOCH 15/20------\n",
            "Training LOSS: 0.8258329236110051 | ACCURACY: 0.6151041666666667\n",
            "------EPOCH 16/20------\n",
            "Training LOSS: 0.8223756910761197 | ACCURACY: 0.6181041666666667\n",
            "------EPOCH 17/20------\n",
            "Training LOSS: 0.819323312441508 | ACCURACY: 0.6201875\n",
            "------EPOCH 18/20------\n",
            "Training LOSS: 0.8164793371160826 | ACCURACY: 0.6220416666666667\n",
            "------EPOCH 19/20------\n",
            "Training LOSS: 0.8137079936067263 | ACCURACY: 0.623375\n",
            "------EPOCH 20/20------\n",
            "Training LOSS: 0.8109742349386215 | ACCURACY: 0.62575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_srnn, test_acc_srnn, test_out_srnn, true_res_lstm, incorrect_examples_srnn, incorrect_labels_srnn, incorrect_pred_srnn = infer(trainer_rnn, simplernn_model, test_dataloader_rnn)"
      ],
      "metadata": {
        "id": "235WXTO5GQ2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Simple RNN Test Loss: {test_loss_srnn}')\n",
        "print(f'Simple RNN Test Accuracy: {test_acc_srnn}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc9t6IQyGqpl",
        "outputId": "9c21c7d2-89be-44d3-97b3-2654a833bcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple RNN Test Loss: 0.8856915869712829\n",
            "Simple RNN Test Accuracy: 0.5800833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gated Recurrent Unit"
      ],
      "metadata": {
        "id": "Mb09C8CxMucd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUNet(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "    super(GRUNet, self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.rnn = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim * 20, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    hidden = self.init_hidden(batch_size).to(device)\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "    out = out.contiguous().view(batch_size, -1)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "  \n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "    return hidden\n"
      ],
      "metadata": {
        "id": "Br8BJtTwQsbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = GRUNet(300, 4, 20, 4).to(device)\n",
        "print(gru_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzYjyhMQYSpB",
        "outputId": "637c3b9b-d3f6-4acd-fb57-4880a32f9c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRUNet(\n",
            "  (rnn): GRU(300, 20, num_layers=4, batch_first=True)\n",
            "  (fc): Linear(in_features=400, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_gru = Trainer(gru_model, train_dataloader_rnn, device)"
      ],
      "metadata": {
        "id": "hudJSccbYZ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses_gru, train_accs_gru) = trainer_gru.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_jBFHe3YoOG",
        "outputId": "2023440c-324e-478d-c7ab-3f1d87f56489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/20------\n",
            "Training LOSS: 0.7165122906366984 | ACCURACY: 0.6806458333333333\n",
            "------EPOCH 2/20------\n",
            "Training LOSS: 0.7113434735139211 | ACCURACY: 0.683875\n",
            "------EPOCH 3/20------\n",
            "Training LOSS: 0.7063109607696533 | ACCURACY: 0.6862916666666666\n",
            "------EPOCH 4/20------\n",
            "Training LOSS: 0.7013799872994423 | ACCURACY: 0.6897916666666667\n",
            "------EPOCH 5/20------\n",
            "Training LOSS: 0.6965669304728508 | ACCURACY: 0.691625\n",
            "------EPOCH 6/20------\n",
            "Training LOSS: 0.6918963541587194 | ACCURACY: 0.6940625\n",
            "------EPOCH 7/20------\n",
            "Training LOSS: 0.6871913316448529 | ACCURACY: 0.696\n",
            "------EPOCH 8/20------\n",
            "Training LOSS: 0.682539682606856 | ACCURACY: 0.6983541666666667\n",
            "------EPOCH 9/20------\n",
            "Training LOSS: 0.6783448173602422 | ACCURACY: 0.69925\n",
            "------EPOCH 10/20------\n",
            "Training LOSS: 0.6741498392621676 | ACCURACY: 0.7018125\n",
            "------EPOCH 11/20------\n",
            "Training LOSS: 0.6705937325755755 | ACCURACY: 0.7029583333333334\n",
            "------EPOCH 12/20------\n",
            "Training LOSS: 0.6670641004244486 | ACCURACY: 0.705625\n",
            "------EPOCH 13/20------\n",
            "Training LOSS: 0.663720713655154 | ACCURACY: 0.7088125\n",
            "------EPOCH 14/20------\n",
            "Training LOSS: 0.6606975847681363 | ACCURACY: 0.7102916666666667\n",
            "------EPOCH 15/20------\n",
            "Training LOSS: 0.6569008466601371 | ACCURACY: 0.7114375\n",
            "------EPOCH 16/20------\n",
            "Training LOSS: 0.6534949411352475 | ACCURACY: 0.7135208333333334\n",
            "------EPOCH 17/20------\n",
            "Training LOSS: 0.6533699630101522 | ACCURACY: 0.7127083333333334\n",
            "------EPOCH 18/20------\n",
            "Training LOSS: 0.6494569268226623 | ACCURACY: 0.7145833333333333\n",
            "------EPOCH 19/20------\n",
            "Training LOSS: 0.648734753270944 | ACCURACY: 0.7143958333333333\n",
            "------EPOCH 20/20------\n",
            "Training LOSS: 0.6457839123408 | ACCURACY: 0.7159375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_gru, test_acc_gru, test_out_gru, true_res_gru, incorrect_examples_gru, incorrect_labels_gru, incorrect_pred_gru = infer(trainer_gru, gru_model, test_dataloader_rnn)"
      ],
      "metadata": {
        "id": "5Ufu2sGBG3vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'GRU Test Loss: {test_loss_gru}')\n",
        "print(f'GRU Test Accuracy: {test_acc_gru}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kBN-UtdKD8k",
        "outputId": "a085c2cb-3690-43f4-964a-97de6182a828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU Test Loss: 0.9679175629615784\n",
            "GRU Test Accuracy: 0.5904166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "96v5xXDFM1MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 20, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size).to(device)\n",
        "        c0 = self.init_hidden(batch_size).to(device)\n",
        "        # print(x.shape)\n",
        "        # print(hidden.shape)\n",
        "        # print(c0.shape)\n",
        "\n",
        "        out, (hidden, c0) = self.lstm(x, (hidden, c0))\n",
        "        out = out.contiguous().view(batch_size, -1)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "Q451kn5eY9Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = LSTMNet(300, 4, 20, 4).to(device)\n",
        "print(lstm_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa-VuNmZZEbJ",
        "outputId": "148931c5-a130-4919-b118-7d818a048908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMNet(\n",
            "  (lstm): LSTM(300, 20, num_layers=4, batch_first=True)\n",
            "  (fc): Linear(in_features=400, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_lstm = Trainer(lstm_model, train_dataloader_rnn, device)"
      ],
      "metadata": {
        "id": "SfWeSSLWZK30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses_lstm, train_accs_lstm) = trainer_lstm.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkhhPgF3ZRXq",
        "outputId": "19b02eb6-8b5b-49f2-fc15-5172d514998c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/20------\n",
            "Training LOSS: 1.0273742792208989 | ACCURACY: 0.4436875\n",
            "------EPOCH 2/20------\n",
            "Training LOSS: 0.9540639110008875 | ACCURACY: 0.5191875\n",
            "------EPOCH 3/20------\n",
            "Training LOSS: 0.9089155247608821 | ACCURACY: 0.554875\n",
            "------EPOCH 4/20------\n",
            "Training LOSS: 0.8778061361710231 | ACCURACY: 0.5779166666666666\n",
            "------EPOCH 5/20------\n",
            "Training LOSS: 0.8557336489756902 | ACCURACY: 0.5902291666666667\n",
            "------EPOCH 6/20------\n",
            "Training LOSS: 0.8389605298042297 | ACCURACY: 0.600625\n",
            "------EPOCH 7/20------\n",
            "Training LOSS: 0.8261014535029729 | ACCURACY: 0.6093958333333334\n",
            "------EPOCH 8/20------\n",
            "Training LOSS: 0.8155284626483917 | ACCURACY: 0.615375\n",
            "------EPOCH 9/20------\n",
            "Training LOSS: 0.8057626006205877 | ACCURACY: 0.6221875\n",
            "------EPOCH 10/20------\n",
            "Training LOSS: 0.7966937431494395 | ACCURACY: 0.6287291666666667\n",
            "------EPOCH 11/20------\n",
            "Training LOSS: 0.7888028825322787 | ACCURACY: 0.6330833333333333\n",
            "------EPOCH 12/20------\n",
            "Training LOSS: 0.7817297620773316 | ACCURACY: 0.6380208333333334\n",
            "------EPOCH 13/20------\n",
            "Training LOSS: 0.7761049771904945 | ACCURACY: 0.6417916666666666\n",
            "------EPOCH 14/20------\n",
            "Training LOSS: 0.7695681476791699 | ACCURACY: 0.6445625\n",
            "------EPOCH 15/20------\n",
            "Training LOSS: 0.7638808170159658 | ACCURACY: 0.6470833333333333\n",
            "------EPOCH 16/20------\n",
            "Training LOSS: 0.7588708358208338 | ACCURACY: 0.6505\n",
            "------EPOCH 17/20------\n",
            "Training LOSS: 0.7534233077764511 | ACCURACY: 0.6531458333333333\n",
            "------EPOCH 18/20------\n",
            "Training LOSS: 0.7487732058763504 | ACCURACY: 0.6556875\n",
            "------EPOCH 19/20------\n",
            "Training LOSS: 0.744391872326533 | ACCURACY: 0.6578125\n",
            "------EPOCH 20/20------\n",
            "Training LOSS: 0.7399819058179855 | ACCURACY: 0.6583125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_lstm, test_acc_lstm, test_out_lstm, true_res_lstm, incorrect_examples_lstm, incorrect_labels_lstm, incorrect_pred_lstm = infer(trainer_lstm, lstm_model, test_dataloader_rnn)"
      ],
      "metadata": {
        "id": "celR2go7FYiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'LSTM Test Loss: {test_loss_lstm}')\n",
        "print(f'LSTM Test Accuracy: {test_acc_lstm}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOrnSNrNF78X",
        "outputId": "80e9dd8f-7d80-42f5-e487-f849f7d6fadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Test Loss: 0.8346940994262695\n",
            "LSTM Test Accuracy: 0.6075\n"
          ]
        }
      ]
    }
  ]
}